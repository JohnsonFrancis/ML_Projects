{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Implementaiton with ANN - Improving accuracy #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.datasets import mnist\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST handwritten digit data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "#Lets do a min max scaling to make it between 0 & 1. This will be good for us in training.\n",
    "X_train=(X_train - X_train.min()) /(X_train.max() - X_train.min())\n",
    "X_test=(X_test - X_test.min()) /(X_test.max() - X_test.min())\n",
    "\n",
    "# Convert y_train into one-hot format \n",
    "y_train_hot = to_categorical(y_train, num_classes=10)\n",
    "y_test_hot = to_categorical(y_test, num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note : model.fit includes an optional argument batch_size. If unspecified, batch_size will default to 32\n",
    "##### 60000/32 = 1875"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                12560     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                170       \n",
      "=================================================================\n",
      "Total params: 13,002\n",
      "Trainable params: 13,002\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.4769 - acc: 0.8605 - val_loss: 0.2571 - val_acc: 0.9225\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2370 - acc: 0.9317 - val_loss: 0.2121 - val_acc: 0.9360\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.1980 - acc: 0.9431 - val_loss: 0.1857 - val_acc: 0.9442\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.1772 - acc: 0.9486 - val_loss: 0.1754 - val_acc: 0.9493\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.1623 - acc: 0.9528 - val_loss: 0.1618 - val_acc: 0.9519\n",
      "Train: 0.958, Test: 0.952\n"
     ]
    }
   ],
   "source": [
    "# Create simple Neural Network model\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(28,28)))      # Input layer 784 neurons\n",
    "model.add(Dense(16, activation='relu'))     # 1st Hidden layer 16 neurons\n",
    "model.add(Dense(16, activation='relu'))     # 2nd Hidden layer 16 neurons\n",
    "model.add(Dense(10, activation='softmax'))   # Output layer 10 neurons\n",
    "model.summary()\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['acc'])\n",
    "# Train the Neural Network model\n",
    "\n",
    "model.fit(X_train, y_train_hot, epochs=5,validation_data=(X_test,y_test_hot))\n",
    "# evaluate the model...Verbose=0 is used to avoid report displays\n",
    "_, train_acc = model.evaluate(X_train, y_train_hot, verbose=0)\n",
    "_, test_acc = model.evaluate(X_test, y_test_hot, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Got 95.1% accuracy with in 5 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now gonna try with different loss functions and optimizaers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creaing lists of optimizers and loss functions\n",
    "loss=['categorical_crossentropy','kl_divergence','poisson','sparse_categorical_crossentropy']\n",
    "opt=['Adadelta','Adagrad','Adam','Adamax','Nadam','RMSprop','SGD']\n",
    "# Creating a dataframe for keeping results\n",
    "df=pd.DataFrame(columns =['Loss','Optimizer','Train Accuracy','Test Accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: categorical_crossentropy, Optimizer: Adadelta, Train: 83.548, Test: 84.530\n",
      "Loss: categorical_crossentropy, Optimizer: Adagrad, Train: 91.267, Test: 91.530\n",
      "Loss: categorical_crossentropy, Optimizer: Adam, Train: 98.948, Test: 95.360\n",
      "Loss: categorical_crossentropy, Optimizer: Adamax, Train: 97.732, Test: 95.660\n",
      "Loss: categorical_crossentropy, Optimizer: Nadam, Train: 99.125, Test: 95.610\n",
      "Loss: categorical_crossentropy, Optimizer: RMSprop, Train: 97.462, Test: 94.890\n",
      "Loss: categorical_crossentropy, Optimizer: SGD, Train: 97.552, Test: 95.480\n",
      "Loss: kl_divergence, Optimizer: Adadelta, Train: 82.328, Test: 82.930\n",
      "Loss: kl_divergence, Optimizer: Adagrad, Train: 91.200, Test: 91.180\n",
      "Loss: kl_divergence, Optimizer: Adam, Train: 98.908, Test: 95.110\n",
      "Loss: kl_divergence, Optimizer: Adamax, Train: 97.693, Test: 95.720\n",
      "Loss: kl_divergence, Optimizer: Nadam, Train: 98.652, Test: 95.210\n",
      "Loss: kl_divergence, Optimizer: RMSprop, Train: 97.952, Test: 95.240\n",
      "Loss: kl_divergence, Optimizer: SGD, Train: 97.752, Test: 95.740\n",
      "Loss: poisson, Optimizer: Adadelta, Train: 81.075, Test: 81.510\n",
      "Loss: poisson, Optimizer: Adagrad, Train: 89.427, Test: 89.930\n",
      "Loss: poisson, Optimizer: Adam, Train: 98.385, Test: 94.770\n",
      "Loss: poisson, Optimizer: Adamax, Train: 97.933, Test: 95.570\n",
      "Loss: poisson, Optimizer: Nadam, Train: 99.352, Test: 95.300\n",
      "Loss: poisson, Optimizer: RMSprop, Train: 97.918, Test: 95.220\n",
      "Loss: poisson, Optimizer: SGD, Train: 94.973, Test: 94.520\n",
      "Loss: sparse_categorical_crossentropy, Optimizer: Adadelta, Train: 84.367, Test: 85.120\n",
      "Loss: sparse_categorical_crossentropy, Optimizer: Adagrad, Train: 91.468, Test: 91.890\n",
      "Loss: sparse_categorical_crossentropy, Optimizer: Adam, Train: 98.908, Test: 95.310\n",
      "Loss: sparse_categorical_crossentropy, Optimizer: Adamax, Train: 97.898, Test: 95.940\n",
      "Loss: sparse_categorical_crossentropy, Optimizer: Nadam, Train: 98.795, Test: 94.830\n",
      "Loss: sparse_categorical_crossentropy, Optimizer: RMSprop, Train: 97.672, Test: 95.420\n",
      "Loss: sparse_categorical_crossentropy, Optimizer: SGD, Train: 96.980, Test: 95.160\n"
     ]
    }
   ],
   "source": [
    "# Looping through all the possibilities with 100 Epocs.. This will run for a few hours.\n",
    "for l in loss:\n",
    "    for o in opt:\n",
    "        model=Sequential([  Flatten(input_shape=(28,28)),\n",
    "                            Dense(16,activation='relu'),\n",
    "                            Dense(16,activation='relu'),\n",
    "                            Dense(10,activation='softmax')  ])                         \n",
    "        model.compile(optimizer=o, loss=l, metrics=['acc'])\n",
    "        if l == 'sparse_categorical_crossentropy': # No one hot encoding required for this\n",
    "            model.fit(X_train,y_train,epochs=100,validation_data=(X_test,y_test),verbose=0)\n",
    "            _, train_acc = model.evaluate(X_train, y_train, verbose=0)\n",
    "            _, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "        else :\n",
    "            model.fit(X_train,y_train_hot,epochs=100,validation_data=(X_test,y_test_hot),verbose=0)\n",
    "            _, train_acc = model.evaluate(X_train, y_train_hot, verbose=0)\n",
    "            _, test_acc = model.evaluate(X_test, y_test_hot, verbose=0)\n",
    "        print('Loss: %s, Optimizer: %s, Train: %.3f, Test: %.3f' % (l,o,train_acc*100, test_acc*100))\n",
    "        df = df.append({'Loss':l,'Optimizer':o,'Train Accuracy':train_acc*100,'Test Accuracy':test_acc*100}, ignore_index=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loss</th>\n",
       "      <th>Optimizer</th>\n",
       "      <th>Train Accuracy</th>\n",
       "      <th>Test Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sparse_categorical_crossentropy</td>\n",
       "      <td>Adamax</td>\n",
       "      <td>97.898334</td>\n",
       "      <td>95.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kl_divergence</td>\n",
       "      <td>SGD</td>\n",
       "      <td>97.751665</td>\n",
       "      <td>95.740002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kl_divergence</td>\n",
       "      <td>Adamax</td>\n",
       "      <td>97.693336</td>\n",
       "      <td>95.719999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>Adamax</td>\n",
       "      <td>97.731668</td>\n",
       "      <td>95.660001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>99.124998</td>\n",
       "      <td>95.609999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>poisson</td>\n",
       "      <td>Adamax</td>\n",
       "      <td>97.933334</td>\n",
       "      <td>95.569998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>SGD</td>\n",
       "      <td>97.551668</td>\n",
       "      <td>95.480001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sparse_categorical_crossentropy</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>97.671664</td>\n",
       "      <td>95.420003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>Adam</td>\n",
       "      <td>98.948336</td>\n",
       "      <td>95.359999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sparse_categorical_crossentropy</td>\n",
       "      <td>Adam</td>\n",
       "      <td>98.908335</td>\n",
       "      <td>95.310003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>poisson</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>99.351668</td>\n",
       "      <td>95.300001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>kl_divergence</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>97.951669</td>\n",
       "      <td>95.240003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>poisson</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>97.918332</td>\n",
       "      <td>95.220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>kl_divergence</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>98.651665</td>\n",
       "      <td>95.209998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sparse_categorical_crossentropy</td>\n",
       "      <td>SGD</td>\n",
       "      <td>96.980000</td>\n",
       "      <td>95.160002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>kl_divergence</td>\n",
       "      <td>Adam</td>\n",
       "      <td>98.908335</td>\n",
       "      <td>95.109999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>97.461665</td>\n",
       "      <td>94.889998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>sparse_categorical_crossentropy</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>98.795003</td>\n",
       "      <td>94.830000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>poisson</td>\n",
       "      <td>Adam</td>\n",
       "      <td>98.385000</td>\n",
       "      <td>94.770002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>poisson</td>\n",
       "      <td>SGD</td>\n",
       "      <td>94.973332</td>\n",
       "      <td>94.520003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>sparse_categorical_crossentropy</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>91.468334</td>\n",
       "      <td>91.890001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>91.266668</td>\n",
       "      <td>91.530001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>kl_divergence</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>91.200000</td>\n",
       "      <td>91.180003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>poisson</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>89.426666</td>\n",
       "      <td>89.929998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>sparse_categorical_crossentropy</td>\n",
       "      <td>Adadelta</td>\n",
       "      <td>84.366667</td>\n",
       "      <td>85.119998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>categorical_crossentropy</td>\n",
       "      <td>Adadelta</td>\n",
       "      <td>83.548331</td>\n",
       "      <td>84.530002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>kl_divergence</td>\n",
       "      <td>Adadelta</td>\n",
       "      <td>82.328331</td>\n",
       "      <td>82.929999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>poisson</td>\n",
       "      <td>Adadelta</td>\n",
       "      <td>81.075001</td>\n",
       "      <td>81.510001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Loss Optimizer  Train Accuracy  Test Accuracy\n",
       "0   sparse_categorical_crossentropy    Adamax       97.898334      95.940000\n",
       "1                     kl_divergence       SGD       97.751665      95.740002\n",
       "2                     kl_divergence    Adamax       97.693336      95.719999\n",
       "3          categorical_crossentropy    Adamax       97.731668      95.660001\n",
       "4          categorical_crossentropy     Nadam       99.124998      95.609999\n",
       "5                           poisson    Adamax       97.933334      95.569998\n",
       "6          categorical_crossentropy       SGD       97.551668      95.480001\n",
       "7   sparse_categorical_crossentropy   RMSprop       97.671664      95.420003\n",
       "8          categorical_crossentropy      Adam       98.948336      95.359999\n",
       "9   sparse_categorical_crossentropy      Adam       98.908335      95.310003\n",
       "10                          poisson     Nadam       99.351668      95.300001\n",
       "11                    kl_divergence   RMSprop       97.951669      95.240003\n",
       "12                          poisson   RMSprop       97.918332      95.220000\n",
       "13                    kl_divergence     Nadam       98.651665      95.209998\n",
       "14  sparse_categorical_crossentropy       SGD       96.980000      95.160002\n",
       "15                    kl_divergence      Adam       98.908335      95.109999\n",
       "16         categorical_crossentropy   RMSprop       97.461665      94.889998\n",
       "17  sparse_categorical_crossentropy     Nadam       98.795003      94.830000\n",
       "18                          poisson      Adam       98.385000      94.770002\n",
       "19                          poisson       SGD       94.973332      94.520003\n",
       "20  sparse_categorical_crossentropy   Adagrad       91.468334      91.890001\n",
       "21         categorical_crossentropy   Adagrad       91.266668      91.530001\n",
       "22                    kl_divergence   Adagrad       91.200000      91.180003\n",
       "23                          poisson   Adagrad       89.426666      89.929998\n",
       "24  sparse_categorical_crossentropy  Adadelta       84.366667      85.119998\n",
       "25         categorical_crossentropy  Adadelta       83.548331      84.530002\n",
       "26                    kl_divergence  Adadelta       82.328331      82.929999\n",
       "27                          poisson  Adadelta       81.075001      81.510001"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display sorted df\n",
    "df.sort_values(['Test Accuracy'],ascending=False,ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We were able to get 95.94% accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
